{
    "config.py->GlobalConfig": {
        "note": "pymarl-compat_sparse_reward",                            // experiment note, also means the log saving directory
        "train_time_testing": "False",                      // do not manage train time testing, pymarl env manage the testing itself
        "heartbeat_on":"False",                             // just some fancy visual effect
        "env_name":"sc2",                                   // starcraft 2
        "env_path":"MISSIONS.starcraft.sc2_env_wrapper",    // starcraft 2
        // "interested_agent_num":100,                         // only for reward logging, **not needed because sc2 use uniform team reward
        "draw_mode": "Img",                                 // plot curlves as image
        "num_threads": "8",                                 // number of parallel envs
        "report_reward_interval": "8",                      // report the reward averaging x episodes
        // "test_interval": "2048",                            // begin a test run every x episodes, test run is managed by pymarl side
        // "device": "cuda:0",
        // "gpu_party": "CUDA0_P1",
        "fold": "1",                                        // each linux process handle x parallel envs
        "seed": 9995,                                       // seed
        "backup_files":[
            // "MISSIONS/sr_tasks/multiagent/scenarios/cargo.py"
        ]

    },

    "MISSIONS.starcraft.sc2_env_wrapper.py->ScenarioConfig": {
        // "map_": "27m_vs_30m",
        "map_": "3m",
        "SINGLE_TEAM_N_AGENT": 3,
        "episode_limit": 60,
        "reward_sparse": true,
        // "reward_vec": true,
        "TEAM_NAMES": [
            "ALGORITHM.pymarl2_compat.pymarl2_compat->PymarlFoundation"
        ]
    },

    "ALGORITHM.pymarl2_compat.pymarl2_compat.py->AlgorithmConfig": {
        "load_checkpoint": "False"
    }
}

